For consistent hashing in distributed systems, I'd explain that it's a technique that minimizes reorganization when nodes are added or removed from a distributed hash table. Traditional hashing distributes keys evenly but requires remapping most keys when the number of nodes changes. Consistent hashing maps both keys and nodes to positions on a conceptual ring, and each key is assigned to the nearest node clockwise around the ring. When a node joins or leaves, only a small fraction of keys need to be remapped. I would mention that virtual nodes (multiple positions for each physical node) help improve distribution and balance load. This approach is fundamental in distributed caches like Memcached and Redis Cluster, distributed databases like Cassandra and DynamoDB, and content delivery networks. Consistent hashing reduces the impact of scaling operations and node failures in distributed systems, making it essential for systems that need to dynamically adapt to changing capacity requirements while minimizing disruption.